---
title: '`FitBenchmarking`: an open source `Python` package comparing data fitting software'
tags:
  - Python
  - fitting
  - non-linear least squares
authors:
  - name: Anders Markvardsen
    affiliation: 1
  - name: Tyrone Rees
    affiliation: 1
  - name: Michael Wathen
    affiliation: 1
  - name: Andrew Lister
    affiliation: 1
  - name: Patrick Odagiu
    affiliation: 1
  - name: Atijit Anuchitanukul
    affiliation: 1
  - name: Tom Farmer
    affiliation: 1
  - name: Anthony Lim
    affiliation: 1
  - name: Federico Montesino
    affiliation: 1
  - name: Tim Snow
    affiliation: 2
  - name:  Andrew McCluskey
    affiliation: 2
affiliations:
 - name: Science and Technology Facilities Council, Rutherford Appleton Laboratory, Harwell Campus, Didcot, Oxfordshire, OX11 0QX
   index: 1
 - name: Diamond Light Source Ltd, Diamond House, Harwell Campus, Didcot, Oxfordshire, OX11 0DE
   index: 2
date: October 2020
bibliography: paper.bib
---
# Summary

Fitting a mathematical model to data is a fundamental task across all scientific disciplines. [`FitBenchmarking`](https://fitbenchmarking.com/) has been designed to help:

* Scientists, who want to know the best algorithm for fitting their data to a given model using specific hardware.
* Scientific software developers, who want to identify the best fitting algorithms and implementations. This allows them to recommend a default solver, to see if it is worth adding a new minimizer, and to test their implementation.
* Mathematicians and numerical software developers, who want to understand the types of problems on which current algorithms do not perform well, and to have a route to expose newly developed methods to users.

Representatives of each of these communities have got together to build `FitBenchmarking`. We hope this tool will help foster fruitful interactions and collaborations across the disciplines.

![Benchmarking paradigm: associating fitting problems represented in individual scientific software packages (top cycle) to optimization software packages (bottom cycle), and bringing these closer together. \label{fig:concept}](figures/FitBenchmarkingConcept.png){width=60%}

`FitBenchmarking` is easy to install via `pip` and our [documentation](https://fitbenchmarking.com/) guides users through the installation of some external packages we support. We provide several data sets from a range of applications and adding new data in these formats is as easy as dropping the data into a new folder. The data and fitting packages currently supported are shown in Figure \ref{fig:concept}. A key part of `FitBenchmarking` is the ease of which a user, with a basic knowledge of `Python`, can add new fitting software, data formats and different fitting comparison output metrics.


# Statement of need

`FitBenchmarking` originally started as a tool to benchmark fitting algorithms in the data reduction package `Mantid` [@mantid], which is used to process neutron scattering and muon spectroscopy data. `FitBenchmarking` has since been significantly extended to take data and models from other real world applications and data analysis / modelling / treatment packages, such as `SasView` [@sasview] and `CUTEst` [@cutest]. It fits models to the data by using a range of data fitting and nonlinear optimization software packages, and present comparisons through a variety of different metrics. These include comparison tables and performance profile plots.

As far as we are aware, `FitBenchmarking` is the only package that is designed
specifically to interface directly with data analysis packages to test different
implemenations of fitting algorithms.  Below we highlight several packages available that
are closest to `FitBenchmarking` in spirit.

* `CUTEst` [@cutest] is written in Fortran, but interfaces
to optimization packages in many languages, offering a way to benchmark any problems
written in a custom SIF format.  However, only the hooks to run the same problem are
provided, the user must provide their own data analysis.
* Paver [@paver] is a package that is part of the COIN-OR initiative.
Paver takes in data that has been generated by running a set of benchmark problems
with a set of solvers as a csv file, and produces an analysis of these (e.g., as
HTML or text tables, or via performance profiles).  In a way that's compliementary to
CUTEst, Paver doesn't come with benchmark problems or link into solvers, but gives a
method to analyse trace file generated by a user's own tests.
* Olympus [@olympus] is a benchmarking framework designed for experiment planning,
which provides analytic benchmark problems, experimental datasets, and emulated datasets.
These benchmark test sets define optimization problems,
on which different optimization methods
(called planners in Olympus) can be tested.  A range of optimization methods are
provided, but the focus is more on testing different algorithms, rather than comparing
the implementations of algorithms.  However, Olympus is designed to add further
problems (called emulators), or optimization methods, so could be used to benchmark
data fitting problems.  
* Benchopt [@benchopt] is a package for general optimization benchmarking, but which
is primarily used to benchmark data fitting using a range of cost functions.
A set of benchmarks on a limited number of datasets are publically available,
and new benchmarks, comprising a data set, cost function, and a set of optimizers
on which to run, can be added. At the end a plot of optimization progress vs time
is produced.

We designed `FitBenchmarking` to compliment and build on these packages, specifically
to benchmark optimization methods applied to data fitting problems.
Specifically, `FitBenchmarking` compares how different fitting algorithms perform for the same data, model and initial guess. The best parameters for the model are found by solving a nonlinear least-squares problem, which can either be solved via a fitting package or an off the shelf minimizer. Figure \ref{fig:sample} displays a data set from `FitBenchmarking` where the crosses are the data points and the two curves are the fits found by two optimization algorithms implemented in `GSL` [@gsl]. From Figure \ref{fig:sample}, it is clear that the solution given by lmsder is better. As the volume of data increases, and we do more and more data analysis algorithmically, it is increasingly important that we have the best algorithm without needing to check it by eye. `FitBenchmarking` generates HTML output that makes it easy to compare minimizers on a given problem set.

![A sample fit: this problem is shipped with `FitBenchmarking`. The data was collected from an instrument named VESUVIO at the ISIS Neutron and Muon Source and has a difficult initial guess. \label{fig:sample}](figures/nmsimplex2_fit_for_EVS14188-90_processed_Gaussian_peaks_1_1.png){width=70%}

`FitBenchmarking` will help the scientist make an informed choice by comparing runtime and accuracy of all available minimizers, on their specific hardware, on problems from their science area.

`FitBenchmarking` will help the scientific software developer ensure that the most robust and quickest algorithms for the type of data analysis they support are available in their software.

`FitBenchmarking` will help mathematicians see what the state of the art is, and what kinds of data are problematic. It will give them access to real data, and will give a route for novel methods to quickly make it into production.

# Acknowledgements

We would like to acknowledge funding support from:

* European Unionâ€™s Horizon2020 research and innovation programme, EU SINE2020 WP-10,
* EPSRC Grant EP/M025179/1 -- Least Squares: Fit for the Future.
* The Ada Lovelace Centre (ALC).

We would also like to thank Nick Draper, Roman Tolchenov, Nick Gould and Jaroslav Fowkes for their helpful comments and advice.

# References
