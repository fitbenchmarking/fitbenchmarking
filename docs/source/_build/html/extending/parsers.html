

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Adding Fitting Problem Definition Types &mdash; FitBenchmarking 0.1.dev1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Adding new cost functions" href="cost_function.html" />
    <link rel="prev" title="Extending FitBenchmarking Documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> FitBenchmarking
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../concept/index.html">Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../users/index.html">Users</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Extending</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Adding Fitting Problem Definition Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="cost_function.html">Adding new cost functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="jacobian_extend.html">Adding new Jacobians</a></li>
<li class="toctree-l2"><a class="reference internal" href="controllers.html">Adding Fitting Software</a></li>
<li class="toctree-l2"><a class="reference internal" href="options_extend.html">Adding new Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="outputs/index.html">Amending FitBenchmarking Outputs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributors/index.html">Contributors</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FitBenchmarking</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Extending FitBenchmarking Documentation</a> &raquo;</li>
        
      <li>Adding Fitting Problem Definition Types</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/extending/parsers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="adding-fitting-problem-definition-types">
<span id="parsers"></span><h1>Adding Fitting Problem Definition Types<a class="headerlink" href="#adding-fitting-problem-definition-types" title="Permalink to this headline">¶</a></h1>
<p>The problem definition types we currently support are listed in the page <a class="reference internal" href="../users/problem_definition_files/index.html#problem-def"><span class="std std-ref">Problem Definition Files</span></a>.</p>
<p>To add a new fitting problem type, the parser name
must be derived from the file to be parsed.
For current file formats by including it as the first line
in the file. e.g <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">Fitbenchmark</span> <span class="pre">Problem</span></code> or <code class="docutils literal notranslate"><span class="pre">NIST/ITL</span> <span class="pre">StRD</span></code>, or by checking
the file extension.</p>
<p>To add a new fitting problem definition type, complete the following steps:</p>
<ol class="arabic">
<li><p>Give the format a name (<code class="docutils literal notranslate"><span class="pre">&lt;format_name&gt;</span></code>).
This should be a single word or string of alphanumeric characters,
and must be unique ignoring case.</p></li>
<li><p>Create a parser in the <code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing</span></code> directory.
This parser must satisfy the following:</p>
<ul class="simple">
<li><p>The filename should be of the form <code class="docutils literal notranslate"><span class="pre">&quot;&lt;format_name&gt;_parser.py&quot;</span></code></p></li>
<li><p>The parser must be a subclass of the base parser, <a class="reference internal" href="../contributors/module_index/fitbenchmarking.parsing.base_parser.html#fitbenchmarking.parsing.base_parser.Parser" title="fitbenchmarking.parsing.base_parser.Parser"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parser</span></code></a></p></li>
<li><p>The parser must implement <code class="docutils literal notranslate"><span class="pre">parse(self)</span></code> method which takes only <code class="docutils literal notranslate"><span class="pre">self</span></code>
and returns a populated <a class="reference internal" href="../contributors/module_index/fitbenchmarking.parsing.fitting_problem.html#fitbenchmarking.parsing.fitting_problem.FittingProblem" title="fitbenchmarking.parsing.fitting_problem.FittingProblem"><code class="xref py py-class docutils literal notranslate"><span class="pre">FittingProblem</span></code></a></p></li>
</ul>
<p>Note: File opening and closing is handled automatically.</p>
</li>
<li><p>If the format is unable to accommodate the current convention of
starting with the <code class="docutils literal notranslate"><span class="pre">&lt;format_name&gt;</span></code>, you will need to edit
<a class="reference internal" href="../contributors/module_index/fitbenchmarking.parsing.parser_factory.html#fitbenchmarking.parsing.parser_factory.ParserFactory" title="fitbenchmarking.parsing.parser_factory.ParserFactory"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParserFactory</span></code></a>.
This should be done in such a way that the type is inferred from the file.</p></li>
<li><p>Create the files to test the new parser.
Automated tests are run against the parsers in FitBenchmarking,
which work by using test files in
<code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing/tests/&lt;format_name&gt;</span></code>.
In the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_parsers.generate_test_cases()</span></code> function,
one needs to add the new parser’s
name to the variable <code class="docutils literal notranslate"><span class="pre">formats</span></code>,
based on whether or not the parser is <code class="docutils literal notranslate"><span class="pre">pip</span></code> installable.
There are 2 types of test files needed:</p>
<ul>
<li><p><strong>Generic tests</strong>: <code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing/tests/expected/</span></code> contains
two files, <code class="docutils literal notranslate"><span class="pre">basic.json</span></code> and <code class="docutils literal notranslate"><span class="pre">start_end_x.json</span></code>.
You must write two input files in the new file format,
which will be parsed using the new parser to check that the entries
in the generated fitting problem match the values expected.
These must be called <code class="docutils literal notranslate"><span class="pre">basic.&lt;ext&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">start_end_x.&lt;ext&gt;</span></code>, where <code class="docutils literal notranslate"><span class="pre">&lt;ext&gt;</span></code>
is the extension of the new file format, and they must be placed in
<code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing/tests/&lt;format_name&gt;/</span></code>.</p></li>
<li><p><strong>Function tests</strong>: A file named <code class="docutils literal notranslate"><span class="pre">function_evaluations.json</span></code>
must also be provided in
<code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing/tests/&lt;format_name&gt;/</span></code>, which tests that the
function evaluation behaves as expected. This file must be in json format and
contain a string of the form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;file_name1&quot;</span><span class="p">:</span> <span class="p">[[[</span><span class="n">x11</span><span class="p">,</span><span class="n">x12</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">x1n</span><span class="p">],</span> <span class="p">[</span><span class="n">param11</span><span class="p">,</span> <span class="n">param12</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">param1m</span><span class="p">],</span> <span class="p">[</span><span class="n">result11</span><span class="p">,</span><span class="n">result12</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">result1n</span><span class="p">]],</span>
                <span class="p">[[</span><span class="n">x21</span><span class="p">,</span><span class="n">x22</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">x2n</span><span class="p">],</span> <span class="p">[</span><span class="n">param21</span><span class="p">,</span> <span class="n">param22</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">param2m</span><span class="p">],</span> <span class="p">[</span><span class="n">result21</span><span class="p">,</span><span class="n">result22</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">result2n</span><span class="p">]],</span>
                <span class="o">...</span><span class="p">],</span>
<span class="p">{</span><span class="s2">&quot;file_name2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">...</span><span class="p">],</span>
 <span class="o">...</span><span class="p">}</span>
</pre></div>
</div>
<p>The test will then parse the files <code class="docutils literal notranslate"><span class="pre">file_name&lt;x&gt;</span></code> in turn evaluate the function
at the given <code class="docutils literal notranslate"><span class="pre">xx</span></code> values and <code class="docutils literal notranslate"><span class="pre">params</span></code>. If the result is not suitably close to
the specified value the test will fail.</p>
</li>
<li><p><strong>Jacobian tests</strong>: <em>If the parser you add has analytic Jacobian
information</em>, then in <code class="docutils literal notranslate"><span class="pre">test_parsers.py</span></code> add
<code class="docutils literal notranslate"><span class="pre">&lt;format_name&gt;</span></code> to the <code class="docutils literal notranslate"><span class="pre">JACOBIAN_ENABLED_PARSERS</span></code> global variable.
Then add a file <code class="docutils literal notranslate"><span class="pre">jacobian_evaluations.json</span></code> to
<code class="docutils literal notranslate"><span class="pre">fitbenchmarking/parsing/tests/&lt;format_name&gt;/</span></code>, which tests that the Jacobian evaluation behaves is as expected.
This file should have the same file structure as <cite>function_evaluations.json</cite>,
and works in a similar way.</p></li>
<li><p><strong>Integration tests</strong>: Add an example to the directory
<code class="docutils literal notranslate"><span class="pre">fitbenchmarking/mock_problems/all_parser_set/</span></code>.
This will be used to verify that the problem can be run by scipy, and that
accuracy results do not change unexpectedly in future updates.
If the software used for the new parser is pip-installable, and the
installation is done via FitBenchmarking’s <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>, then add the
same example to <code class="docutils literal notranslate"><span class="pre">fitbenchmarking/mock_problems/default_parsers/</span></code>.</p>
<dl>
<dt>As part of this, the <code class="docutils literal notranslate"><span class="pre">systests/expected_results/all_parsers.txt</span></code> file,</dt><dd><p>and if neccessary the <code class="docutils literal notranslate"><span class="pre">systests/expected_results/default_parsers.txt</span></code> file,
will need to be updated. This is done by running the systests:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pytest</span> <span class="n">fitbenchmarking</span><span class="o">/</span><span class="n">systests</span>
</pre></div>
</div>
</dd>
</dl>
<p>and then checking that the only difference between the results table and the
expected value is the new problem, and updating the expected file with the result.</p>
</li>
</ul>
</li>
<li><p>Verify that your tests have been found and are successful by running
<cite>pytest -vv fitbenchmarking/parsing/tests/test_parsers.py</cite></p></li>
</ol>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="cost_function.html" class="btn btn-neutral float-right" title="Adding new cost functions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="Extending FitBenchmarking Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, STFC.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>